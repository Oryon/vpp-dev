# Segment Routing based Load Balancer    {#srlb_plugin_doc}

***
/!\
This document contains Cisco confidential information that should only be disclosed under NDA.
/!\
***

The Segment Routing Load Balancer is a specific type of load balancer leveraging IPv6 Segment Routing in order to dispatch new connection requests (e.g. TCP SYNs) to an ordered list of candidate servers, which in turn decide based on local policy, traffic, and system monitoring, whether to accept the connection or let it go to the next candidate.

The list of candidate servers for a given flow is computed using an algorithm derived from Google's *Maglev* consistent hashing load-balancer. Consistent hashing in general, and *Maglev* in particular, provide good resiliency to deny of service attacks and enable multiple load-balancers to be deployed in parrallel without state synchronization.

The use of multiple candidate servers improves upon the results presented in *The Power of Choices in Load Balancing* by moving the decision process from the Load Balancer (typically infering servers load by using the connection count), to the servers themselves (using local and real-time performance metrics).

SRLB therefore combines load-aware load-balancing with consistent hashing algorithms in order to provide simultaneously good resiliency and fairness.

This document focuses on the implementation and usability aspects of SRLB VPP Plugin. Architectural and protocol details may be found here:

- [The Power of Two Choices in Randomized Load Balancing](https://www.eecs.harvard.edu/~michaelm/postscripts/tpds2001.pdf)
- [Google's Maglev](https://research.google.com/pubs/pub44824.html)
- [SRLB: The Power of Choices in Load Balancing with Segment Routing](http://ieeexplore.ieee.org/document/7980143/)
- [6LB: Scalable and Application-Aware Load Balancing with Segment Routing](http://ieeexplore.ieee.org/document/8293698/)

### VPP Plugin Maintainers
- Pierre Pfister <ppfister@cisco.com>
- Yoann Desmouceaux <ydesmouc@cisco.com>

## Overview

SRLB operations are split between two agents.

- The *Load Balancer*, implemented by the *srlb* plugin, receives incoming packets from clients and dispatch them to candidate or selected servers using IPv6 Segment Routing.
- The *Server Agent*, implemented by the *srlb_agent* plugin, inserts and removes IPv6 SR headers from packets coming from and going to the applications, and takes the decision of accepting a new connection request sent to multiple candidates.

The lifetime of a flow is as follows:

1. When the first packet of a given flow arrives to the Load-Balancer, a segment routing header is inserted in the packet such as to send it to an ordered list of server candidate.
2. Each server receiving the packet may either *accept* the connection based on local policy decision making and deliver the packet locally, without the IPv6 SR header, or *reject* it and forward it to the next candidate server in the IPv6 SR list.
3. At this point, further packets coming from the client are sent to the same list of server candidates.
4. When return traffic is generated by the application, the Server Agent inserts an SR header, steering the packet toward the Load Balancer which steered the first packet(s).
5. Such packets arriving at the Load Balancer are stripped of their SR header and forwarded to the client. It also tells the Load Balancer which server accepted the connection.
6. Further packets from the client are forwarded by the Load Balancer directly to the server which accepted the connection.
7. At that point, the Server Agent knows that the Load Balancer is capable of forwarding packets directly to the right server, without going through the candidate list. This is the end of the *in-band hand-shake* between the Load-Balancer and the Server Agent.
8. Further packets from the application are sent directly to the client (The Load-Balancer does not need to forward most of the return traffic).
9. When a FIN packet is forwarded by the Server Agent, it is, again, sent through the Load Balancer, such as to inform it that the flow is soon to be removed. This reduces the lifetime of the flow in the Load Balancer flow table.

## Load Balancer

### Virtual IP and Segment Routing Prefixes

The basic Load Balancing configuration entity is the Virtual IP, or VIP for short, identified by an *IPv6 prefix of any length*. Packets coming from clients which destination address is included in a VIP Prefix will be load-balanced to the set of servers configured for this particular VIP. A single VPP Load Balancer may be configured with an arbitrary number of different VIPs.

Each VIP must be assigned a unique *80 bits long IPv6 Prefix* which is used to receive packets with a Segment Routing header originated by Server Agents. Such prefixes must be different for each VIP, and must be properly routed by the Server Agents, and the intermediate network nodes, toward the Load Balancer. In practice, a given Load Balancer would be provisioned with a 64 bits long prefix such as to be able to serve up to 2^16 VIPs. A larger prefix, or multiple prefixes, could be provisioned in case more VIPs should be configured.

The CLI command used to add or delete VIPs is `srlb lb vip`.

For example, in order to add a load-balancing service for traffic going to the prefix `2001:db8:1111:1::/64`, and using the IPv6 Prefix `2001:db8:ffff:1:1::/80` for IPv6 SR operations:

```
srlb lb vip 2001:db8:1111:1::/64 sr 2001:db8:ffff:1:1::/80
```

The VIP may later be deleted with the following command:

```
srlb lb vip 2001:db8:1111:1::/64 del
```

### Server Pools and Consistent Hashing algorithm

Each VIP is configured with a list of server pools. When a packet for a new flow is received from a client, a Segment Routing header is inserted with as many server candidates as non-empty pools configured for the corresponding VIP. The first candidate server is from the first non-empty pool, the second candidate is selected from the second non-empty pool, etc...

This approach is generic enough to accomodate various scenarios. For instance:

- A typical SRLB deployment would configure two pools containing the same sets of servers. Server candidates are then equally positioned as first or second choices.
- A purely consistent-hashing based approach would be to configure a single pool, hence having a single candidate for new connections. Such a setup would behave in a way that is similar to *Maglev*.
- One may imagine configuring 3 or more pools with the same sets of servers in order to increase the load balancing fairness, but research showed that most benefits comes from the second choice.
- Some servers might be dedicated to situations of high demand (e.g. providing a reduced service requiring less ressources). In such a case, one would configure two pool with distinct sets of servers.

Everytime the set of servers and pools for a given VIP is modified, a new consistent hash table is generated. This table, which size must be a power of two (configured on a per VIP basis), provides a mapping between flow hashes and server candidates lists. By default, the hash value is computed based on the flow 5-tuple (src/dst IP, protocol number and src/dst port) but the VIP may also be configured to compute the hash based on VIP address only. The latter is only usefull when the VIP is a pretty large prefix with sufficient diversity in the incoming traffic, and is typically usefull in cases where it is desireable that a given VIP address (/128) is sent to the same server.

The consistent hash table size can be configured using the command (Here with a size of 1024 buckets):

```
srlb lb vip 2001:db8:1111:1::/64 consistent-hash-size 1024
```

The hashing algorithm can also be configured using a similar CLI call (Here with 'vip', but '5-tuple' is also supported):

```
srlb lb vip 2001:db8:1111:1::/64 hash vip
```

Those parameters may be configured simultaneously (here at the creation of the VIP):

```
srlb lb vip 2001:db8:1111:1::/64 sr 2001:db8:ffff:1:1::/80 consistent-hash-size 2048 hash 5-tuple
```


From a CLI perspective, it is possible to add or delete batches of servers from a specific set of pools (the consistent hashing table is recomputed only once per batch). Sets of pools are encoded as a comma seperated list of either integers `n` or `n-m` where `n` and `m` are integers, in which case all pools from index `n` to `m` (included) are in the pool set. The current implementation supports pool indexes from 0 to 31 (i.e. more addresses than would realistically be set in an IPv6 SR List).

As explained later in this document, each server is also identified by a 80 bits long prefix used to carve up IPv6 SR SIDs. For instance, in order to add servers `2001:db8:ffff:2:1::/80` and `2001:db8:ffff:3:1::/80` to pools `1`, `2`, `3` and `5` of the VIP `2001:db8:1111:1::/64`, one would use the command:

```
srlb lb server 2001:db8:1111:1::/64 1-3,5 add 2001:db8:ffff:2:1::/80 2001:db8:ffff:3:1::/80
```

One could later remove server `2001:db8:ffff:2:1::/80` from pool `1` and `2`, and server `2001:db8:ffff:3:1::/80` from pools `3` and `5`.

```
srlb lb server 2001:db8:1111:1::/64 1-2 del 2001:db8:ffff:2:1::/80
srlb lb server 2001:db8:1111:1::/64 3,5 del 2001:db8:ffff:3:1::/80
```

### Using different FIBs (VRFs)

The Load-Balancer interacts with the outside in four different ways:

1. Receiving packets from clients, which destination is included in a VIP prefix.
2. Receiving packets from *server agents*, which destination is an SRLB SR SID.
3. Sending packets to the clients.
4. Sending packets to the *server agents*

Each of these operation require an IP Lookup (although 4. is not performed on the data-path, but rather during FIB updates). By default, the four operations are performed using the default FIB (table 0), but in some cases it is desirable to perform those operations in different FIBs. For example:

- When routes to the clients and to the *server agents* are configured in different VRFs (1. and 3. are performed in a different FIB from 2. and 4.)
- When a single VIP must behave in different ways depending on the interface the packets come from.

In order to handle those situations, each of the previous four operations can be performed in different FIBs, configured on a per VIP basis, using the following `srlb lb vip` options.

- `client-rx-table-id <n>`: Table used to receive packets which destination is included in the VIP prefix.
- `client-tx-table-id <n>`: Table used to send packet to clients.
- `sr-rx-table-id <n>`: Table used to receive packets to the VIP SRLB SR prefix.
- `sr-tx-table-id <n>`:Table used to send packets to the *server agents* SRLB SR prefixes.

In addition, it is worth noting that SRLB VIPs using different FIBs to receive packets from clients are seen as totally different VIPs, which may be configured independently (e.g. with different servers, table size, hashing algorithm, or even number of servers in the hunting path). For that purpose, the `client-rx-table-id <n>` argument also exists in the `srlb lb server` command in order to disambiguate between different VIP instances using the same prefix.

For example, the following configuration is valid:
```
ip6 table add 1

srlb lb vip 2001:1::/64 sr 2001:ffff:2::
srlb lb vip 2001:1::/64 client-rx-table-id 1 sr 2001:ffff:1::

srlb lb server 2001:1::/64 1 add 2001:ffff:b::
srlb lb server 2001:1::/64 1-2 add client-rx-table-id 1 2001:ffff:a::
```

### Flow-Table

In addition to the per VIP Consistent Hashing table, the Load-Balancer maintains one flow table per worker thread, shared by all VIPs.

Conceptually, all flows are associated with one of the following states:

- **Listen**: No actual state is kept in the flow table.
- **Hunting**: The flow table point to the server candidate list from the consistent hash table which was used to send the received SYN packets.
- **Steer**: The flow table points to the server which accepted the connection.
- **Teardown**: Same as steer, but with a smaller timeout increment.

More information about the flow table may be found in the [Flow Table](#flow-table) section.

### *show* commands

The Load Balancer plugin implements a few commands to help monitor the system.

- `show srlb lb conf` displays global configuration parameters.
- `show srlb lb counters` displays global counters.
- `show srlb lb flows [verbose <level>]` displays information related to the flow table.
- `show srlb lb vip [vip-prefix] [verbose <level>]` displays information related to the VIPs and servers.

Note that `flows` and `vip` will show greater verbose information as the verbosity level increases. The verbosity can also be expressed using the following formats: `verbose`, `verbose <level>`, `very-verbose`, or `v<level>`.


## Server Agent

### Application instances

The basic Server Agent configuration object is called an Application instance and is identified by a unique *80 bits long IPv6 prefix* that is used for SR communications with the Load Balancers. A given VPP instance may be configured with an arbitrary number of Application instances, each of which being associated with:

- An IPv6 destination address used to determine the adjacency that should be used when delivering packets locally. For instance, this address may be: 
	- A next-hop address in the case of a local VM accessed through a virtual interface
	- A locally configured address in the case of a container using VPP TCP Stack
	- An address which resolved recursive route points toward a tunnel virtual interface.

- An *accept policy*, which describes how the process of accepting a new connection is performed. The policy is identified by a policy name (e.g., `always-accept`, `always-reject`, `lru`) which internally corresponds to a given function, and an opaque number that is used to differentiate instances of the same policy type.

In order to create or delete an Application instance, the following commands are used:

```
srlb sa application 2001:db8:ffff:2:1::/80 via 2001:db8:aaaa:3::1 policy always-accept 0
srlb sa application 2001:db8:ffff:2:1::/80 del
```

Interestingly enough, a given Application does not need to be configured with the VIP prefix. Instead, the unique 80 bits long prefix that is used by the Load Balancer when forwarding incoming traffic toward the server is leveraged to identify the Application instance associated with incoming packets. When a connection is accepted locally, an entry is added in the flow table for the given flow. A given Application instance may therefore be used for multiple different VIP Prefixes.

When a packet is sent by the Application instance toward the original client, it must be processed by the dedicated SRLB SA Application VPP node in order to update the flow timeout and check whether it should be sent through a Load Balancer by inserting an SR header, or directly to the client. Such operation is performed by enabling the `srlb-sa-app` interface input feature in all interfaces where such packets may come from (e.g. a virtio interface when the application is within a VM).

Enabling and disabling the `srlb-sa-app` feature on a given interface is done with the following commands:

```
set int feature <interface-name> srlb-sa-app arc ip6-unicast
set int feature <interface-name> srlb-sa-app arc ip6-unicast disable
```

In multi-threaded environments, it is important that incoming and outgoing traffic go through the same worker thread. For that purpose, the operator must make sure that all the traffic coming from a given application is processed by a given worker thread, using the `set int rx-placement` command. The Application instance may then be associated with a given thread such that traffic coming from the Load Balancer is automatically handed-off to the corresponding thread:

```
srlb sa application 2001:db8:ffff:2:1::/80 via 2001:db8:aaaa:3::1 policy always-accept 0 thread 2
```

### Flow-Table

The Server Agent maintains one flow table per worker thread, shared by all Application instances.

Conceptually, all flows are associated with one of the following states:

- **None**:  No actual state is kept in the flow table.
- **Wait**: Return traffic is sent back through the load balancer, which address is kept in the flow table.
- **Direct**: Return traffic is sent back directly to the client.
- **Teardown**: Return traffic is sent back through the load balancer in order to inform that the flow is terminating.

More information about the flow table may be found in the [Flow Table](#flow-table) section.

### *show* commands

Additional information to monitor the system may be obtained through the various *show* commands.

- `show srlb sa accept-policies` lists the different available accept policies. More information may be found in the [Accept Policies](#accept-policies) section of this document.
- `show srlb sa conf` shows global configuration parameters.
- `show srlb sa counters` shows global Server Agent counters.
- `show srlb sa flows [verbose <n>]` shows flow table information.
- `show srlb sa applications [<sr-prefix>]` shows Application instances information and counters.


## Flow Table

The Load Balancer (resp. Server Agent) uses a dedicated flow table that is shared by all configured VIPs (resp. applications). The flow table implements lazy timeouts, efficient flow insertion, lookups and may store millions of flows without performance degradation.

By default, any activity will set the flow timeout to 40 seconds, except in the teardown state, in which case the timeout is set to 10 seconds. Both values may be modified with system-wide parameters using the following commands:

```
srlb lb conf active_timeout 20 teardown_timeout 5
srlb sa conf active_timeout 30 teardown_timeout 15
```

The size of the flow table is configured using two parameters:

- Number of *fixed entries*: Number of buckets in the hash table. Each entry may store zero or one flow, but also be chained with a *collision entry*. The number of fixed entries must be a power of two.
- Number of *collision entries*: A collision entry contains 16 entries, and may therefore store up to 16 flow states. It is used in case multiple flow hashes collision to the same fixed entry.

In order to efficiently store up to N flows per core, a rule of thumb is to allocate `8*N` fixed entries, and `N/8` collision entries.

For example, using the following command:

```
srlb lb conf fixed_entries 8388608 collision_buckets 131072
srlb sa conf fixed_entries 524288 collision_buckets 8192
```

More information about the flow table itself may be found in the flow table source code header file `src/vppinfra/flowhash_template.h`.


## Debug Capabilities

Both Load Balancer and Server Agent implement logging mechanism with different log levels (`error`, `warning`, `info`, `debug` and `data`) and may be set using the following commands:

```
debug srlb lb warning
debug srlb sa debug
```

The data log level is extremly verbose (typically displays event for each flow), and is only compiled in debug mode.

## Application Instances Accept Policies

Each Application instance is associated with a single Accept Policy instance. Such an instance is identified by an Accept Policy name (a.k.a. type) and an opaque instance number. The different implemented Accept Policies may be listed with the `show srlb sa accept-policies` command.

### Test and dummy policies

The following policies are implemented for test and simple deployment purposes, and may be configured using any opaque value.

- `always-accept`: A new connection request is always accepted.
- `always-reject`: A new connection request is always rejected.
- `only-last`: A new connection request is only accepted when the Application instance is last in the packet's candidate list.

### Nginx and Apache policies

The SRLB Server Agent plugin implements Accept Policies plugging into Apache or Nginx shared memory scoreboard. Those policies are called *apache-threashold* and *nginx-threshold* and accept new connections requests whenever the number of HTTP busy threads is below or equal to a certain threashold (or, as last resort, when the instance is last in the SR list).

Instances of such policies are created with the following commands:

```
srlb sa apache-policy opaque <opaque> [scoreboard <scoreboard_file>] [threshold <threshold>] [namespace_pid <pid>] [del]
srlb sa nginx-policy opaque <opaque> [pid <pid>] [threshold <threshold>] [del]
```

In this case, the policy instance id, called `opaque`, is provided by the user. The same value must be provided when configuring the `accept-policy` of a given application in order to bind it with the right policy instance. The `opaque` parameter must be specified upon policy creation and deletion.

When creating the policy, the `threshold` parameter *must* be set, to the desired threshold above which connections are refused.

For the Apache policy, the `scoreboard` parameter *must* be set when creating the policy, and should referer to the shared memory file in which Apache publishes metrics about current connections. This must match the `ScoreBoardFile` parameter of Apache's `httpd.conf` file. The `namespace_pid` is an _optional_ parameter specifying the PID of the Apache process, if it is running inside a namespace (_e.g.,_ if using Docker containers). This allows VPP to enter the corresponding namespace and fetch the shared memory file from there.

For the Nginx policy, unfortunately nginx does not provide a named shared memory to read statistics from, but does use an internal, anonymous shared memory. Hence, the `pid` parameter *must* be set to nginx's PID when creating the policy. VPP will then try to map nginx internal shared memory by inspecting the open pages of the process. Note that the PID as viewed from the host namespace must be provided, regardless of whether nginx is running in a separate namespace or not.

Here is an example of how to set a Docker container with Apache, and bind it to an SRLB Apache policy:
```
$ cat > Dockerfile << EOF
FROM httpd
RUN echo "ScoreBoardFile scoreboard" >> /usr/local/apache2/conf/httpd.conf
EOF
$ docker build -t my-apache2 .
$ docker run --name apache2-test my-apache2
$ PID=$(docker inspect -f '{{.State.Pid}}' apache2-test)
$ vppctl srlb sa apache-policy opaque 123 scoreboard /usr/local/apache2/scoreboard threshold 4 namespace_pid $PID
$ vppctl srlb sa application 2001:db8:ffff:2:1::/80 via 2001:db8:aaaa:3::1 policy apache-threshold 123
```
In addition, it is necessary to add an interface in the container and connect it to VPP. Although several methods exist, here is an example of how this can be done, with a tap interface ``tapapache0``:
```
$ sudo mkdir /var/run/netns
$ sudo ln -s /proc/$PID/ns/net /var/run/netns/$PID
$ VPPTAP=$(vppctl tap connect tapapache0)
$ vppctl set interface ip address $VPPTAP 2001:db8:aaaa:3::2/64
$ vppctl set interface state $VPPTAP up
$ sudo ip link set tapapache0 netns $PID
$ sudo ip netns exec $PID ifconfig tapapache0 add 2001:db8:aaaa:3::1/64 up
$ sudo ip netns exec $PID ip -6 route add default via 2001:db8:aaaa:3::2
$ sudo rm /var/run/netns/$PID
```

### CPU threshold policy

The CPU threshold policy can be used for applications for which a simple metric such as the number of active connections is unavailable, but which are CPU intensive. It is notably useful in a container context, wherein an application or a group of applications shares a _CPU accounting_ (`cpuacct`) cgroup. With this policy, queries are accepted if and only if the CPU usage of the group is below a certain threshold.

This policy is instantiated as follows:
```srlb sa cpuacct-policy opaque <opaque> filename <cpuacct-file> threshold <thresh> [del]```
The policy identifier (to be used in the `srlb sa application policy` command) must be specified through the `opaque` parameter. In addition, `cpuacct-file` must contain the path of a _cpuacct_ file, from which the plugin will retrieve CPU usage information. Finally, `thresh` is a threshold parameter, representing the CPU usage (in percents) above which queries are refused. Note that, in multi-cores environment, a CPU usage of 100\% represents one fully-used core, and thus it might be necessary to specify thresholds above 100\%. 

The system-wide _cpuacct_ file is `/sys/fs/cgroup/cpuacct/cpuacct.usage`. When using Docker containers, the _cpuacct_ file for a given container `$CONTAINER_ID` can be found at `/sys/fs/cgroup/cpuacct/docker/$CONTAINER_ID/cpuacct.usage`.


### Least Recently Used (lru) policies

The Least Recently Used policy is meant for contexts where requests are made toward a large set of different IPv6 VIP addresses, and where the application should only accept connections for the most popular VIP addresses.

A *Least Recently Used filter* is used in order to decide whether a given VIP address is popular enough to be accepted. Such filter can be seen as a list of elements. Every time a connection request is performed for a certain VIP address, the address is looked-up in the LRU list. If the element is found early enough in the list, it is considered popular, the connection is accepted, and the element is moved to the very-top of the list. If the element is not found, or is found too deep in the LRU, the connection is rejected, but the element is still put on top of the list. As a consequence, a given connection will be accepted whenever a previous request was received in a recent timeframe.

In order to define what *popular* means, every VIP address is associated with a cost which is derived from the VIP address with the following formula:
```
cost = (vip & mask)*2^exp + add
```

It is for instance possible to assign a cost of `n` to all VIP addresses by using parameters such as `mask=::` and `add=n`, or encode the cost in the lower order 16 bits of the VIP address, in kilobytes, with a minial cost of one, by using the parameters: `mask=::ffff`,  `exp=10` and `add=1`.

When a new connection request is received, the connection is accepted if the VIP address is found in the LRU and if the total cost of all elements that are *before* in the LRU list (i.e. for which a request have been received after the last time a request for the given VIP address was received), plus the cost of the considered VIP address, is smaller than the configured threshold.

A given LRU policy instance may be created or deleted using the following command:

```
srlb sa lru-policy [size <ip6-address-mask> <exp> <add>] [threshold <threshold>] [entries <n>]
srlb sa lru-policy index <n> del
```

Where the `threshold` is the total cost after which, in the LRU, a content is not considered popular aymore; and the `entries` integer corresponds to the expected maximum number of popular entries at any given time.
